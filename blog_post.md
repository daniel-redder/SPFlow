### _What are Sum-Product Networks?_

     **Sum-product networks (SPNs)** are a subcategory of mathematical models that have a wide variety of applications on complex systems, mainly probabilistic reasoning and other Bayesian inference. Probabilistic inference is the process of determining the probability of certain events occurring on the basis of observed data.  Graphical models represent the relationships between random variables, and allow the calculation of different event likelihoods from probability distributions. Sum-product networks get their name from their structure, a combination of sum and product nodes. Sum nodes represent the "sum" operation and allow the SPN to capture the inherent uncertainty or randomness in the data. Product nodes, on the other hand, represent the "product" operation and capture the dependencies between different random variables. 

_SPN Structure Example (Poon & Domingos 2012)_

![](https://lh6.googleusercontent.com/X2j2iEcky7x2syfIKyepIoyn3ZJgIAGWm0X0BXqcmht7np5XblGBuvninzu8MMQXsuOA_ZrkTNT6AwQD6Fr__sHgm7dSESqIDhrj-_mDUUdKd98YyP3mEvVcOgZ6vDpyVnj7D_8AbQ_3bHWduU2cLcvA2Aa1S9ZHkVB0-5-tSzcbdSdXMduSENaWRieOeA)

    By combining these two types of nodes, an SPN can be used to compactly represent a complex probability distribution and perform efficient inference on that distribution. The edges of the network encode the relationships between the variables, through the usage of a directed acyclic graph to represent the functions. The output of a sum-product network is computed by performing a series of multiplications and weighted sums, following the structure of the graph. Encoding the operations facilitates the efficient computation of the probability of different events occurring in the system with linear complexity, making sum-product networks a powerful tool for rapid reasoning under uncertainty. SPN’s have key properties that allow for efficient inference, including decomposability, consistency, validity, and completeness.

_SPN Properties_

<table><tbody><tr><td>Property</td><td>Function</td></tr><tr><td>Decomposability</td><td>Allows SPNs to be divided into a set of sub-SPNs, each of which are individually valid.&nbsp;</td></tr><tr><td>Consistency</td><td>Ensures that the probabilities generated by the SPN match the true probabilities of the distribution variables. According to Poon &amp; Domingos (Poon &amp; Domingos 2012), consistency is encompassed by decomposability.</td></tr><tr><td>Validity</td><td>Ensures that the probability distribution defined by the SPN is. A valid SPN is one where the probabilities associated with each node and edge are non-negative, and sum up to 1. A valid SPN is complete.</td></tr><tr><td>Completeness</td><td>An SPN is complete if it can represent any distribution over the modeled variables. This is done by representing the joint probability distribution as a product of individual conditional distributions.&nbsp;</td></tr></tbody></table>

### _SPFlow_

    SPFlow is an open-source Python library for developing SPNs. Tools are provided for defining and training network structure, probability distributions, and even optimization algorithms for model parameters. After generating an SPN, SPFlow also provides functions for performing probabilistic inferences. These inferences include computing marginalization, log-likelihoods, conditional sampling, and classification through most probable explanation (MPE). To get started with SPFlow, here are some code blocks from the SPFlow documentation for basic SPN creation and inference. 

    Using DSL, an SPN with categorical leaves can be defined as follows:

```python

from spn.structure.leaves.parametric.Parametric import Categorical
spn = 0.4 * (Categorical(p=[0.2, 0.8], scope=0) *
             (0.3 * (Categorical(p=[0.3, 0.7], scope=1) *
                     Categorical(p=[0.4, 0.6], scope=2))
            + 0.7 * (Categorical(p=[0.5, 0.5], scope=1) *
                     Categorical(p=[0.6, 0.4], scope=2))))
    + 0.6 * (Categorical(p=[0.2, 0.8], scope=0) *
             Categorical(p=[0.3, 0.7], scope=1) *
             Categorical(p=[0.4, 0.6], scope=2))
```

    Using object hierarchy, an SPN with categorical leaves can be defined as follows:

```python
from spn.structure.leaves.parametric.Parametric import Categorical
from spn.structure.Base import Sum, Product		
from spn.structure.Base import assign_ids, rebuild_scopes_bottom_up

p0 = Product(children=[Categorical(p=[0.3, 0.7], scope=1), Categorical(p=[0.4, 0.6], scope=2)])
p1 = Product(children=[Categorical(p=[0.5, 0.5], scope=1), Categorical(p=[0.6, 0.4], scope=2)])
s1 = Sum(weights=[0.3, 0.7], children=[p0, p1])
p2 = Product(children=[Categorical(p=[0.2, 0.8], scope=0), s1])
p3 = Product(children=[Categorical(p=[0.2, 0.8], scope=0), Categorical(p=[0.3, 0.7], scope=1)])
p4 = Product(children=[p3, Categorical(p=[0.4, 0.6], scope=2)])
spn = Sum(weights=[0.4, 0.6], children=[p2, p4])

assign_ids(spn)
rebuild_scopes_bottom_up(spn)
return spn
```

    To perform inference, here’s an example of marginalizing an SPN and calculating the log likelihood of the marginalized SPN:

```python
from spn.algorithms.Marginalization import marginalize
from spn.algorithms.Inference import log_likelihood
spn_marg = marginalize(spn, [1,2])

llm = log_likelihood(spn_marg, test_data)
print(llm, np.exp(llm))
```

### _SPN Extensions and Applications_

   SPNs have extensions to support more complex inference. Dynamic SPNs (DSPNs) represent models that change over time, and are well suited for temporal dependencies between variables. They are particularly useful when multiple processes interact, or when latent variables are not directly observed. Sum-Product Max Networks (SPMNs) compute the values of variables that maximize the posterior probability, given evidence, using maximum a posteriori (MAP) estimates. SPMNs are able to perform probabilistic and MAP inference, to extend SPNs to decision making. There are various flavors implemented to these extensions, such as Recurrent SPNs (RSPNs) and Recurrent SPMNs (RSPMNs). RSPNs combine recurrent neural networks with SPNs, and are suited for variables that are dependent on the previous variables in the sequence. The recurrent aspect utilizes neural networks, allowing feedback connections to model dependencies between variables by computing hidden states at each time step.  

    A common application of SPNs is in natural language processing by modeling the probability of word occurrences. This is useful in tasks related to speech recognition and machine translation, particularly when the goal is to determine the most likely sequence of words. This application may be extended to automatically generate text that is similar to an input. SPNs may also be used in computer vision in modeling the probability of different objects appearing in an image. This application can help in image segmentation, in which the goal is to identify and classify different image objects, or object detection, where the goal is to locate and identify image objects. Moreso, SPNs can be employed in computer vision to automatically generate novel images that are related to the input. In regard to artificial intelligence, SPNs can be deployed in planning and decision making. Through modeling the probabilities of various events occurring as a result of observed data, an optimal series of an agent’s actions can be determined to achieve an overarching goal. Other applications of SPNs include medical diagnosis, fraud detection, bioinformatics, and other pattern recognition and probabilistic domains.

### _Arithmetic Circuits vs Sum-Product Networks_

 **Arithmetic circuits (ACs)** are another graphical model used for representing and computing functions in a way that is both efficient and mathematically rigorous. These models are similar in that they both use a graphical structure to represent a mathematical function, and the nodes in the graph correspond to operations that are performed on the inputs to the function. There are a few key differences between SPNs and ACs. SPNs are a type of probabilistic graphical model, meaning that they are used in representing probability distributions over a set of random variables. Arithmetic circuits are deterministic models, insofar that they always compute the same output for a given set of inputs. A key difference between the models is that SPNs are restricted to sum and product operations as the basic building blocks of computation, while ACs may use a more general set of operations including multiplication and addition. As a result of this, ACs can be more expressive in their computation of a wider range of functions in comparison to SPNs. 

    While SPNs and ACs are similar, they are applied for different purposes and have some differences in their mathematical properties and expressive power. Both SPNs and ACs are used in a variety of applications, such as machine learning and computational complexity theory. SPNs are generally considered to be more efficient than ACs, because they can exploit the structure of the function being computed in order to reduce the computational complexity. Arithmetic circuits are used for performing complex mathematical computations, while sum-product networks are used for performing probabilistic inference and other types of statistical computations.

 _Logic Diagram of an Example Arithmetic Circuit (Sarangi et. al 2014)_

![](https://lh6.googleusercontent.com/UQrIPC4gDao206VIppgcqGFM9Ojsrt62tEUl2neNkP4up_cK22LgiVrliexgx9zzAgkHsI9C4QrrTRHKwZhIBV-AfJECFxLr5jMlPgyHSFTi_j5jzYMxQovkDyt460upIK71Uy9-h-L2HeJvvS3L6VV0BdFBQ_slqD-bsNT1AN8_9iTo93cZ5ogWXNA2Sw)
